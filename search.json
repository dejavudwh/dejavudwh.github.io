[{"title":"muduo源码解析：TcpServer","date":"2022-05-08T04:13:05.000Z","url":"/2022/05/08/muduo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9ATcpServer/","tags":[["muduo","/tags/muduo/"],["源码解析","/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"]],"categories":[["muduo源码解析","/categories/muduo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"]],"content":"继上篇分析完Channel、Acceptor、EPollPoller和部分EventLoop之后，已经能够大致理解了整个muduo的流程。 回顾上篇最后提到的问题：谁来负责创建和传递Channel？TcpConnection。 TcpConnectionTcpConnection负责管理一个TCP连接和这个连接的输入输出缓冲区，由Acceptor接收到一个新连接时创建，也就是上节讲到的Acceptor::handleRead中的回调所创建的，下面会详细解析。 先来看它的部分接口定义： 其中TcpConnection中的callback都是由用户设置（TcpServer为一个间接层，后面再提），也就是说用户要处理的三个半事件都是由TcpConnection进行管理，当然TcpConnection还持有一个Channel，也就是最后还是由TcpConnection反应到Channel中（在构造函数中设置）。 当然这些callback还需要TcpConnection再进行一层wrap，因为用户只关心逻辑的处理，所以还是有一些工作需要TcpConnection来完成。TcpConnection对这些callback的wrap就是在handleRead、handleWrite等中。 handleWrite、handleRead等的逻辑都非常简单，主要都是判断channel的状态然后通过Buffer对fd进行读写，Buffer是muduo中提供的一个缓冲区，这个后面再解析。所以这里我们只看handleWrite，首先判断channel是否可读，如果可读则调用sockets::write，这个函数也只是对writed的一层封装，然后通过write的返回值判断Buffer是否被读完，来以此决定要不要回调writeCompleteCallback以及关闭连接。 sendInLoop当然，TcpConnection除了封装了handleWrite等，还向用户提供send接口： 这里涉及了muduo中和EventLoop有关的非常重要的设计。 首先，这里分为两种情况： 在对应的EventLoop线程中 如果在对应的线程当中，直接调用sendInLoop进行发送，主要逻辑和handleWrite有点类似，也是根据channel和buffer的不用状态来走向不同的分支，这里不同的是，如果还有未写完的数据，需要重新写入outBuffer中，并且还要判断是否需要调用highWaterMarkCallback_。 不在对应的EventLoop线程中 如果不在对应的线程中，则会调用EventLoop的runInLoop，加入队列，由上篇没有讲的doPendingFunctors进行处理。我们先来看一下代码： 这里的swap也是一个小技巧，来减小临界区的范围。也就是说，这里如果不在对应EventLoop的线程当中，即会唤醒对应的线程来处理相应的任务。这样的作用是当其他线程需要通过这个线程的资源来执行任务的时候，并不是直接再其他线程中访问资源调用函数，这样就会造成资源的竞争，需要加锁保证，而现在我们让当前线程为其他线程提供一个接口，其他线程将要执行的任务用这个接口交给当前线程这样当前线程统一处理自己的资源，而不用加锁，唯一需要加锁的地方就是通过接口添加任务的任务队列这个地方，大大减小了锁粒度。 这里为什么有竞态？如果我们允许任务在其它线程执行，则它对应的EventLoop可能此时会同一个Channel会有新事件发送，则对TcpConnection的调用可能会发送竟态。 但是这时候我们注意到，为什么sendInLoop还是会调用queueInLoop呢？此时它已经是在对应的IO线程中的。👇 原文链接 这时候我们就可以来看一下TcpServer，也是muduo暴露给用户最重要的一个类。 TcpServer经过前面的铺垫，TcpServer就变得非常简单了，先来看一下它的部分接口： 我们可以看到它主要持有一个Acceptor和一个TcpConnection的Map，还有一个线程池，线程池的内容我们放在下一篇。所以根据前面的讲解，我们已经可以猜到TcpServer的工作了，通过Acceptor来接受一个新连接，并传入回调来创建一个TcpConnection和其对应的Channel，然后在TcpServer中分配TcpConnection到不同的EventLoop中（多线程下）。 newConnection我们来看一下具体代码是不是这样做的，先来看一下TcpServer的构造函数和它传给Acceptor的回调： 和我们猜测的流程一样，而对于TcpConnection设置的回调，其实就是用户所传入的，当然在TcpConnection中再继续把回调传入Channel时，还是会有一层wrap的，这是上面提到的。 这时候我们回过来关注刚刚没有提到的connectEstablished，这也是理解muduo生命周期的一个关键。 connectEstablished现在我们可以来看一下一个TcpConnection的生命周期了 创建： 由TcpServer传入Acceptor的回调中创建，并放入Map当中 释放： 当一个tcp连接关闭的时候，首先触发Poller的可读事件，调用Channel::handledEvent来处理，然后调用TcpConnection::handledread处理，handledRead经过read后发现返回字节为0，继续调用TcpCOnnecton::handledClose，在这里面会调用tcpServer注册的回调函数removeConnection，TcpServer::removeConnection会将这个TcpConnection从它的连接列表中删除，但是此时不能直接delete TcpConnection这个对象，因为TcpConnnection对象中的channel还在函数调用栈中调用handledEvent，此时如果直接delete TcpConnection，会将channel也delete掉，程序会出现core dump错误，所以需要等channel::handeledEvent，执行完毕之后再delete TcpConnection对象，这就用到shared_ptr。 所以，这时候我们也可以回过头来看Channel中的tie和tied_了，它在connectEstablished中被绑定到至少和TcpConnectio的生命周期一样长： 当TCP连接断开，这个IO事件会触发Channel::handleEvent()函数，而后者回调用户提供的CloseCallbask，用户代码可能可能析构Channel对象。这样就会引起Channel::handleEvent()执行到一半时，其所属的Channel被销毁了。 Buffer这篇的最后一个内容，Buffer的设计非常简洁，只要理解了Buffer的结构就可以很好的理解这些接口。 其中prependable的作用就是让程序以很低的代价在数据前面添加几个字节。比如，程序以固定的4个字节表示消息的长度来对Tcp进行分包，其它结构的含义则非常显而易见。 总结现在muduo的整个流程已经非常清晰了，我们通过TcpServer设置相应事件的回调函数，然后TcpServer持有的Acceptor则用来接受新连接，并且将自己的Channel通过EventLoop注册到Poller中，而TcpServer传给Acceptor的回调在每次接受新连接时都会被调用，而后创建一个TcpConnection，TcpConnection则创建一个Channel并且通过EventLoop进行事件注册，如果在多线程下，每次创建新的TcpConnection时，都会分配到各自的EventLoop中。最后EventLoop在自身的loop中不断等待事件和调用Channel处理事件以及处理runInLoop添加的任务。 下篇我们开始分析线程池。"},{"title":"muduo源码解析：EventLoop","date":"2022-05-07T13:36:01.000Z","url":"/2022/05/07/muduo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AEventLoop/","tags":[["muduo","/tags/muduo/"],["源码解析","/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"]],"categories":[["muduo源码解析","/categories/muduo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"]],"content":"本系列文章主要对muduo的核心源码进行解析。 在正式看muduo源代码之前，必须先理解Reactor模型。 Reactor模型1服务器端程序一般要处理三类事件：IO事件、信号及定时事件，由此延申出来的就有两种关于如何处理这三类事件的事件处理模型：Reactor和Proactor。这里主要讨论Reactor。 Reactor模型本质上主线程*（main reactor)*只负责监听Socket fd上是否有事件发生，有的立即通知工作线程进行处理，也就是说接受新连接、读写数据，业务逻辑均在工作线程中。 使用同步IO模型（以epoll为例）实现的Reactor模型的工作流程是： 主线程往epoll内核事件表中注册socket上的读就绪事件。 主线程调用epoll_wait等待socket上有数据可读。 当socket上可读事件发生时，epoll_wait通知主线程，主线程将socket可读事件放入请求队列，由工作线程进行消费。 工作线程从socket上读取数据，处理业务逻辑，然后往epoll内核事件表上注册该socket上的写就绪事件。 主线程调用epoll_wait等待socket可写。 当socket上可写事件发生时，epoll_wait通知主线程，主线程将socket可写事件放入请求队列，由工作线程进行消费。 工作线程向socket写入响应消息。 所以Reactor模型一般包含几个组件：句柄（Handle)、事件多路分发器（EventDmultiplexer)、事件处理器（EventHandler）和Reactor。 Reactor是整个框架的核心，Reactor执行事件循环，由用户通过Reactor向事件多路分发器注册事件，句柄也就是统一事件源，当内核检测到就绪事件时，它将通过句柄来通知事件，再由事件多路分发器通过epoll_wait等来分发事件到Reactor，最后再由Reactor调用相应的事件处理器。 muduo的事件模型muduo的事件模型是one loop per thread + threadpool。 也就是说，在多线程下，muduo的每一个子线程都会有一个由mainReactor创建的subReactor，也就是一个事件循环，而其中的被分配到subReactor的socket中的事件和业务逻辑也都由subReactor的子线程进行处理。 所以在多线程下，muduo的工作流程和上面所说的流程的主要区别是： 主线程的事件循环中只监听新连接的事件，并且将新连接根据Round-Robin分配到subReactor中的事件循环。 在此后由该socket触发的事件包括计算任务都由当前subReactor进行处理。 Channel和事件循环相关的类主要有Channel、Acceptor、Poller、EventLoop。Channel是其它类的基础，所以先来分析一个Channel这个类。 Channel主要是对fd和统一事件源及其对应的事件处理的封装。1 Channel的成员变量如下，意义都比较明显，index_和tied_后面分析。 Channel的成员函数最重要的就是事件处理函数handleEvent和设置和取消所关注的事件的一些系列函数enable...()和disable...()。 handleEvent我们要知道，Channel封装的是fd和事件及其事件处理回调，所以显然handleEvent会被在例如epoll_wait返回之后的事件循环中调用，看一下源码： tie和tied_在这里就发挥了作用，它的作用就是延长Channel的生命周期，详细我们仍然按住不表，到后面分析TCPConnection的时候再看。 所以这里会继续调用handleEventWithGuard，逻辑也会很清晰，即根据事件来调用回调函数。1 update和remove接下来比较重要的函数就是update和remove，根据函数名字显然易见，一个是更新关注的事件，一个是移除当前Channl（也就是不关注所有事件了）。 其中update函数在enable...()和diable...()系列函数之后调用，也就是先对更改事件，然后再真正的去内核事件表中进行更新，省掉一些重复代码。 可以看到这里更新和删除Channel最后都是调用EventLoop的函数，其中EventLoop再去调用Poller提供的接口。所以接下来看一下Poller类。 Poller在muduo里提供了epoll和poll来做多路复用，默认使用epoll。所以Poller类是这两个类的父类，这里我们主要关注EPollPoller。 EPollPoller的设计非常简洁，接口如下： 接口的含义都非常明显，其中poll就是相当于调用epoll_wait等待事件返回，由EventLoop调用，然后通过fillActiveChannels去填充EventLoop传进来的activeChannels，而EventLoop则根据这个List去处理回调。 先来看removeChannel操作： 逻辑非常的清晰，先从Poller所持有的channels_删除当前Channel，再从epoll中删除当前channel关注的事件。这里就可以体现之前Channel中index的作用了，主要用来标志当前的Channel是新添加、被删除还是一个正在监听的Channel。注意，这里的remove并不会真正的删除channel，这里后面再解释。 再看updateChannel和update： 而updateChannel的核心就是根据channel来对fd更新事件，其实也就是根据index来判断。而update就是完成最后一步的工作，调用epoll_ctl去设置和更新事件，并且通过event.data.ptr保存Channel。 最后来看其中的核心fillActiveChannels： 完成的工作很简单，其实就是通过epoll_wait拿到事件集合，然后将其中event.data.ptr保存的Channel指针放入EventLoop传过来的activeChannels。 所以现在我们根据Reactor模型，就可以猜测EventLoop到底在做什么：我们通过EventLoop向Poller注册Channel，然后在EventLoop的事件循环当中，通过Poller的poll调用epoll_wait等待事件，然后通过返回的activeChannels进行事件回调。 这是行得通的，但是在muduo中为了更好的使用Channel，所以又对Channel封装了Acceptor、Connector、TCPConnection，在这篇文章里只先分析Acceptor。 Acceptor这个类名的含义也显而易见，究其根源就是封装了Channel，最后调用了accpet来接收一个新的连接。 来先看它的接口： 接口的定义也非常清晰，其中loop就是当前acceptChannel所持有的那个loop，而Socket本质上也就是对linux socket api的一个封装。 我们先来看Acceptor的constructor和listen： constructor和listen 总的来说，就是创建一个Socket，并且通过这个Socket创建一个Channel，并且设置相关参数。在这里向acceptChannel设置读事件回调，并且在调用了listen之后向EventLoop和Poller注册了事件和事件回调。 handleRead很显然这里的事件回调就是处理新连接到达之后的操作： 根据一开始我们讲解的muduo的事件处理模型，这里应该会将不同的连接分入不同的subReactor，但是这里是调用了创建类时传入的newConnectionCallback_，因为在这个位置是一个很重要的处理位置，也就是三个半事件中的一个，所以应该处理用户传入的回调。当然这里的newConnectionCallback_也是被wrap过的，所以它依旧会将不同的连接分入不同的subReactor。这里在后面看TCPServer的时候再详细解释。 所以现在我们的准备工作都已经做完了，可是正式来看EventLoop了。 EventLoopEventLoop是muduo中最重要的一个类，因为里面涉及了一些设计，所以代码也相对的复杂。 所以，我们先看这篇文章中涉及的部分，之后其它内容在后续文章呈现。 首先来看它的接口设计（省略一大部分）： 其中updateChannel、removeChannel、hasChannel都是直接调用Poller提供的接口，在之前Poller部分也分析过了，大家可以自行看代码。 所以，我们可以直接看EventLoop最核心的事件循环loop，有了之前的讲解，对loop的理解会非常简单： 很显然，loop不断通过Poller::poll获得此时触发事件的Channel，然后直接调用Channel的handleEvent进行处理。但是这里有一个doPendingFunctors，它被用来处理计算任务和定时任务等，详细且听下篇分解。 总结：所以这么一细看来，muduo的类设计和逻辑都非常清晰。我们简单的总结一下： Channel本质上就是对fd和event以及event handler的封装，Acceptor则是对Channel的封装，主要功能就是来负责新连接的accept。 而EPollPoller则是epoll相关操作的封装，我们通过传递Channel到Poller中来对关注的事件进行设置和更新，而在EventLoop中的loop则主要是不断通过Poller::poll获得此时触发事件的Channel，然后直接调用Channel的handleEvent进行处理。 所以这里最后的疑问就是：谁来负责创建和传递Channel呢？且看下篇分解。"},{"title":"about","date":"2022-05-06T09:17:12.000Z","url":"/about/index.html","categories":[[" ",""]],"content":"dejavudwh,"},{"title":"category","date":"2022-05-06T09:16:54.000Z","url":"/categories/index.html","categories":[[" ",""]]},{"title":"about","date":"2022-05-06T09:17:12.000Z","url":"/404.html","categories":[[" ",""]],"content":"dejavudwh,"},{"title":"search","date":"2022-05-08T03:41:46.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"tags","date":"2022-05-08T03:46:41.000Z","url":"/tags/index.html","categories":[[" ",""]]}]